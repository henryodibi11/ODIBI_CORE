# LEVEL 8: Putting It All Together

**Time**: 60+ minutes  
**Goal**: Build complete production-ready pipelines with best practices and real-world patterns

---

## ğŸ¯ What You'll Learn

- Complete pipeline patterns (Medallion, Streaming, Cloud)
- Production best practices checklist
- Troubleshooting guide
- 4 complete production-ready examples
- How to combine all features

---

## Complete Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PRODUCTION PIPELINE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER DEFINES STEPS                                             â”‚
â”‚ steps = [Step(...), Step(...), ...]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATOR SETUP                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚ Engine     â”‚  â”‚ Tracker    â”‚  â”‚ EventBus   â”‚                â”‚
â”‚ â”‚ Context    â”‚  â”‚ (lineage)  â”‚  â”‚ (hooks)    â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚Checkpoint  â”‚  â”‚ Cache      â”‚  â”‚ Metrics    â”‚                â”‚
â”‚ â”‚ Manager    â”‚  â”‚ Manager    â”‚  â”‚ Manager    â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DAGEXECUTOR / DISTRIBUTEDEXECUTOR                              â”‚
â”‚ â”œâ”€ Parallel execution (THREAD_POOL / PROCESS_POOL)            â”‚
â”‚ â”œâ”€ Cache lookup before execution                              â”‚
â”‚ â”œâ”€ Checkpoint after each layer                                â”‚
â”‚ â”œâ”€ Retry on failure (max 3 times)                             â”‚
â”‚ â””â”€ Emit events for monitoring                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST-EXECUTION                                                 â”‚
â”‚ â”œâ”€ Export metrics (JSON + Prometheus)                         â”‚
â”‚ â”œâ”€ Generate HTML story                                        â”‚
â”‚ â”œâ”€ Trigger completion hooks                                   â”‚
â”‚ â””â”€ Save final checkpoint                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Production Best Practices Checklist

### âœ… Configuration

- [ ] Use meaningful step names (not "step1", "step2")
- [ ] Set appropriate engine (Pandas for <10GB, Spark for >10GB)
- [ ] Configure retry logic (`max_retries=3`)
- [ ] Set checkpoint mode (`CheckpointMode.LAYER` for medallion)
- [ ] Define secrets management strategy

### âœ… Reliability

- [ ] Enable checkpoints for long-running pipelines
- [ ] Enable cache for development environments
- [ ] Configure retry with exponential backoff
- [ ] Set up error alert hooks
- [ ] Test failure scenarios and resume logic

### âœ… Observability

- [ ] Enable Tracker for all production pipelines
- [ ] Register error alert hooks (Slack, email, PagerDuty)
- [ ] Export metrics to monitoring system
- [ ] Generate execution stories for debugging
- [ ] Set up dashboard for pipeline metrics

### âœ… Data Quality

- [ ] Validate data schemas
- [ ] Check row counts (alert on 0 rows)
- [ ] Monitor data freshness
- [ ] Track schema changes over time
- [ ] Set up data quality tests

### âœ… Security

- [ ] Never hardcode secrets in code
- [ ] Use `context.get_secret()` for credentials
- [ ] Store secrets in Key Vault / Secrets Manager
- [ ] Don't log sensitive data
- [ ] Use managed identities when possible

### âœ… Performance

- [ ] Use Parquet for large datasets
- [ ] Partition data appropriately
- [ ] Enable distributed execution for independent nodes
- [ ] Use Delta Lake for ACID guarantees
- [ ] Monitor memory usage

### âœ… Operations

- [ ] Document pipeline dependencies
- [ ] Set up scheduled execution if needed
- [ ] Configure alerting for failures
- [ ] Test rollback procedures
- [ ] Maintain runbooks for common issues

---

## Common Pipeline Patterns

### Pattern 1: Medallion Architecture (Bronze â†’ Silver â†’ Gold)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MEDALLION ARCHITECTURE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ BRONZE (Raw Data)                                       â”‚
â”‚   â”œâ”€ IngestNode: Read from source                      â”‚
â”‚   â”œâ”€ StoreNode: Save as-is (Parquet/Delta)            â”‚
â”‚   â””â”€ Checkpoint: After Bronze layer                    â”‚
â”‚                                                         â”‚
â”‚ SILVER (Cleaned Data)                                   â”‚
â”‚   â”œâ”€ TransformNode: Clean, dedupe, validate           â”‚
â”‚   â”œâ”€ TransformNode: Add business logic                â”‚
â”‚   â”œâ”€ StoreNode: Save cleaned (Delta recommended)      â”‚
â”‚   â””â”€ Checkpoint: After Silver layer                    â”‚
â”‚                                                         â”‚
â”‚ GOLD (Aggregated/Business-Ready)                        â”‚
â”‚   â”œâ”€ TransformNode: Aggregate, join, enrich           â”‚
â”‚   â”œâ”€ TransformNode: Business calculations              â”‚
â”‚   â”œâ”€ StoreNode: Save aggregated (Delta)               â”‚
â”‚   â””â”€ PublishNode: Export to BI tools                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pattern 2: Streaming/Continuous

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STREAMING PIPELINE                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ StreamManager (INCREMENTAL mode)                       â”‚
â”‚   â”œâ”€ Read data > watermark                            â”‚
â”‚   â”œâ”€ Process batch                                     â”‚
â”‚   â”œâ”€ Update watermark                                  â”‚
â”‚   â”œâ”€ Save checkpoint (iteration N)                    â”‚
â”‚   â””â”€ Loop forever                                      â”‚
â”‚                                                         â”‚
â”‚ OR: File Watch Mode                                    â”‚
â”‚   â”œâ”€ Monitor landing directory                        â”‚
â”‚   â”œâ”€ New file appears â†’ Process                       â”‚
â”‚   â”œâ”€ Move to processed/                               â”‚
â”‚   â””â”€ Wait for next file                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pattern 3: Cloud Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLOUD PIPELINE                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Azure ADLS Source                                       â”‚
â”‚   â”œâ”€ CloudAdapter: Authenticate (managed identity)    â”‚
â”‚   â”œâ”€ IngestNode: Read from ADLS                       â”‚
â”‚   â””â”€ Spark engine for large data                      â”‚
â”‚                                                         â”‚
â”‚ Transform (Distributed)                                 â”‚
â”‚   â”œâ”€ DistributedExecutor (THREAD_POOL)                â”‚
â”‚   â”œâ”€ Complex transformations                           â”‚
â”‚   â””â”€ Multiple parallel branches                        â”‚
â”‚                                                         â”‚
â”‚ Write Back to Cloud                                     â”‚
â”‚   â”œâ”€ StoreNode: Delta Lake on ADLS                    â”‚
â”‚   â”œâ”€ Partitioned by date                              â”‚
â”‚   â””â”€ ACID guarantees                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pattern 4: Scheduled Batch

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHEDULED BATCH PIPELINE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ ScheduleManager (CRON: "0 2 * * *")                   â”‚
â”‚   â””â”€ Trigger daily at 2:00 AM                         â”‚
â”‚                                                         â”‚
â”‚ Full Pipeline Execution:                                â”‚
â”‚   â”œâ”€ Read yesterday's data                            â”‚
â”‚   â”œâ”€ Transform and aggregate                          â”‚
â”‚   â”œâ”€ Save to data warehouse                           â”‚
â”‚   â”œâ”€ Generate reports                                 â”‚
â”‚   â””â”€ Send completion notification                     â”‚
â”‚                                                         â”‚
â”‚ Error Handling:                                         â”‚
â”‚   â”œâ”€ Retry up to 3 times                              â”‚
â”‚   â”œâ”€ Send alert if all retries fail                   â”‚
â”‚   â””â”€ Save checkpoint for manual recovery              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complete Production Examples

### Example 1: Simple Development Pipeline

**Scenario**: Local CSV processing for testing

```python
from odibi_core.step import Step
from odibi_core.orchestrator import Orchestrator

# Steps
steps = [
    # 1. Read CSV
    Step(
        layer="ingest",
        name="read_sales_csv",
        type="config_op",
        engine="pandas",
        value="data/sales.csv",
        params={"source_type": "csv", "header": True},
        outputs={"data": "raw_sales"}
    ),
    
    # 2. Filter high-value sales
    Step(
        layer="transform",
        name="filter_high_value",
        type="sql",
        engine="pandas",
        value="SELECT * FROM bronze WHERE amount > 100",
        inputs={"bronze": "raw_sales"},
        outputs={"data": "filtered_sales"}
    ),
    
    # 3. Add tax column
    Step(
        layer="transform",
        name="add_tax",
        type="sql",
        engine="pandas",
        value="SELECT *, amount * 0.1 as tax FROM bronze",
        inputs={"bronze": "filtered_sales"},
        outputs={"data": "sales_with_tax"}
    ),
    
    # 4. Save to Parquet
    Step(
        layer="store",
        name="save_parquet",
        type="config_op",
        engine="pandas",
        value="output/sales_processed.parquet",
        params={"format": "parquet"},
        inputs={"data": "sales_with_tax"}
    )
]

# Orchestrator (development config)
orchestrator = Orchestrator(
    steps=steps,
    engine_type="pandas",
    enable_tracker=True,
    enable_cache=True,  # Speed up dev cycles
    cache_dir="cache/"
)

# Execute
result = orchestrator.execute()

# Generate story
result['tracker'].export_lineage("lineage.json")
from odibi_core.story_generator import StoryGenerator
story_gen = StoryGenerator()
story_gen.generate_story("lineage.json", "story.html")

print(f"âœ… Pipeline completed: {result['success']}")
print(f"ğŸ“Š Story generated: story.html")
```

---

### Example 2: Production Medallion Pipeline

**Scenario**: Bronze â†’ Silver â†’ Gold with full observability

```python
from odibi_core.step import Step
from odibi_core.orchestrator import Orchestrator
from odibi_core.checkpoint_manager import CheckpointMode
from odibi_core.event_bus import EventBus, EventPriority
from odibi_core.metrics_manager import MetricsManager

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEPS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
steps = [
    # === BRONZE LAYER ===
    Step(
        layer="bronze",
        name="ingest_customers",
        type="config_op",
        engine="spark",
        value="abfss://raw@storage.dfs.core.windows.net/customers.csv",
        params={"source_type": "csv", "header": True},
        outputs={"data": "raw_customers"}
    ),
    
    Step(
        layer="bronze",
        name="save_bronze_customers",
        type="config_op",
        engine="spark",
        value="data/bronze/customers",
        params={"format": "delta", "mode": "overwrite"},
        inputs={"data": "raw_customers"}
    ),
    
    # === SILVER LAYER ===
    Step(
        layer="silver",
        name="clean_customers",
        type="sql",
        engine="spark",
        value="""
            SELECT 
                customer_id,
                TRIM(UPPER(name)) as name,
                LOWER(email) as email,
                age,
                status
            FROM bronze 
            WHERE customer_id IS NOT NULL 
              AND email IS NOT NULL
        """,
        inputs={"bronze": "raw_customers"},
        outputs={"data": "clean_customers"}
    ),
    
    Step(
        layer="silver",
        name="dedupe_customers",
        type="sql",
        engine="spark",
        value="""
            SELECT * FROM (
                SELECT *, 
                       ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY age DESC) as rn
                FROM bronze
            ) WHERE rn = 1
        """,
        inputs={"bronze": "clean_customers"},
        outputs={"data": "dedupe_customers"}
    ),
    
    Step(
        layer="silver",
        name="save_silver_customers",
        type="config_op",
        engine="spark",
        value="data/silver/customers",
        params={"format": "delta", "mode": "overwrite", "partitionBy": ["status"]},
        inputs={"data": "dedupe_customers"}
    ),
    
    # === GOLD LAYER ===
    Step(
        layer="gold",
        name="aggregate_by_status",
        type="sql",
        engine="spark",
        value="""
            SELECT 
                status,
                COUNT(*) as customer_count,
                AVG(age) as avg_age,
                CURRENT_TIMESTAMP() as processed_at
            FROM bronze
            GROUP BY status
        """,
        inputs={"bronze": "dedupe_customers"},
        outputs={"data": "customer_summary"}
    ),
    
    Step(
        layer="gold",
        name="save_gold_summary",
        type="config_op",
        engine="spark",
        value="data/gold/customer_summary",
        params={"format": "delta", "mode": "overwrite"},
        inputs={"data": "customer_summary"}
    )
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OBSERVABILITY SETUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
metrics = MetricsManager()
event_bus = EventBus()

# Alert on errors
def error_alert(event_data):
    print(f"ğŸš¨ CRITICAL: {event_data['step_name']} failed!")
    print(f"   Error: {event_data['error_message']}")
    # In production: Send to Slack/PagerDuty

event_bus.register_hook("node_error", error_alert, EventPriority.CRITICAL)

# Log layer completions
def log_layer_complete(event_data):
    if event_data.get('layer_complete'):
        print(f"âœ… Layer '{event_data['layer']}' completed")

event_bus.register_hook("node_complete", log_layer_complete)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ORCHESTRATOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
orchestrator = Orchestrator(
    steps=steps,
    engine_type="spark",
    
    # Reliability
    enable_checkpoints=True,
    checkpoint_mode=CheckpointMode.LAYER,  # Checkpoint after each layer
    checkpoint_dir="checkpoints/",
    max_retries=3,
    retry_backoff=True,
    
    # Observability
    enable_tracker=True,
    metrics_manager=metrics,
    event_bus=event_bus,
    
    # Performance
    distributed=True,
    distributed_max_workers=10
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXECUTE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
result = orchestrator.execute()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POST-EXECUTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Export metrics
metrics.save_to_file("metrics.json")
metrics.export_prometheus("metrics_prometheus.txt")

# Generate story
result['tracker'].export_lineage("lineage.json")
from odibi_core.story_generator import StoryGenerator
story_gen = StoryGenerator()
story_gen.generate_story("lineage.json", "medallion_story.html")

print(f"\n{'='*60}")
print(f"Pipeline Status: {'âœ… SUCCESS' if result['success'] else 'âŒ FAILED'}")
print(f"Nodes Executed: {result['nodes_executed']}")
print(f"Duration: {metrics.get_summary()['total_duration_ms']}ms")
print(f"{'='*60}")
```

---

### Example 3: Streaming File Processor

**Scenario**: Watch directory, process new files continuously

```python
from odibi_core.step import Step
from odibi_core.stream_manager import StreamManager, StreamMode, StreamConfig

# Steps (same as batch)
steps = [
    Step(
        layer="ingest",
        name="read_events",
        type="config_op",
        engine="pandas",
        value="data/landing/",  # Will be replaced by StreamManager
        params={"source_type": "csv", "header": True},
        outputs={"data": "raw_events"}
    ),
    
    Step(
        layer="transform",
        name="filter_important",
        type="sql",
        engine="pandas",
        value="SELECT * FROM bronze WHERE priority = 'HIGH'",
        inputs={"bronze": "raw_events"},
        outputs={"data": "important_events"}
    ),
    
    Step(
        layer="store",
        name="append_to_processed",
        type="config_op",
        engine="pandas",
        value="data/processed/events.parquet",
        params={"format": "parquet", "mode": "append"},
        inputs={"data": "important_events"}
    )
]

# Stream config
stream_config = StreamConfig(
    source_path="data/landing/",
    file_pattern="*.csv",
    format="csv",
    batch_size=None  # Process entire file
)

# Stream manager
stream_manager = StreamManager(
    steps=steps,
    engine_type="pandas",
    stream_mode=StreamMode.FILE_WATCH,
    stream_config=stream_config,
    checkpoint_dir="stream_checkpoints/",
    enable_tracker=True
)

# Start (runs forever)
print("ğŸ”„ Starting file watcher...")
print("Drop CSV files into data/landing/ to process them")
stream_manager.start()
```

---

### Example 4: Scheduled Cloud Pipeline

**Scenario**: Daily Azure ADLS â†’ Transform â†’ Delta, scheduled

```python
from odibi_core.step import Step
from odibi_core.orchestrator import Orchestrator
from odibi_core.schedule_manager import ScheduleManager, ScheduleMode
from odibi_core.cloud_adapter import CloudAdapter
from odibi_core.checkpoint_manager import CheckpointMode

# Cloud adapter
adapter = CloudAdapter.create(
    provider="azure",
    account_name="mystorageaccount",
    use_managed_identity=True  # Databricks
)

# Steps
steps = [
    Step(
        layer="ingest",
        name="read_azure_sales",
        type="config_op",
        engine="spark",
        value="abfss://raw@storage.dfs.core.windows.net/sales/{date}.parquet",
        params={"source_type": "parquet", "cloud_adapter": adapter},
        outputs={"data": "raw_sales"}
    ),
    
    Step(
        layer="transform",
        name="aggregate_daily",
        type="sql",
        engine="spark",
        value="""
            SELECT 
                DATE(sale_date) as date,
                product_id,
                SUM(amount) as total_amount,
                COUNT(*) as sale_count
            FROM bronze
            GROUP BY DATE(sale_date), product_id
        """,
        inputs={"bronze": "raw_sales"},
        outputs={"data": "daily_summary"}
    ),
    
    Step(
        layer="store",
        name="save_delta",
        type="config_op",
        engine="spark",
        value="abfss://processed@storage.dfs.core.windows.net/sales_summary",
        params={
            "format": "delta",
            "mode": "append",
            "partitionBy": ["date"],
            "cloud_adapter": adapter
        },
        inputs={"data": "daily_summary"}
    )
]

# Pipeline function
def run_daily_pipeline():
    orchestrator = Orchestrator(
        steps=steps,
        engine_type="spark",
        enable_checkpoints=True,
        checkpoint_mode=CheckpointMode.LAYER,
        enable_tracker=True,
        max_retries=3
    )
    
    result = orchestrator.execute()
    
    if not result['success']:
        print(f"âŒ Pipeline failed: {result['failed_nodes']}")
        # Send alert
    else:
        print(f"âœ… Pipeline completed successfully")

# Schedule daily at 2:00 AM
scheduler = ScheduleManager()
scheduler.schedule(
    mode=ScheduleMode.CRON,
    cron_expression="0 2 * * *",  # Daily at 2 AM
    func=run_daily_pipeline,
    name="daily_sales_pipeline"
)

print("ğŸ“… Scheduler started - will run daily at 2:00 AM")
scheduler.start()
```

---

## Troubleshooting Guide

| Problem | Likely Cause | Solution |
|---------|-------------|----------|
| **Pipeline fails mid-execution** | Transient error | Enable checkpoints, resume with `resume_from_checkpoint=True` |
| **Slow repeated runs during dev** | Re-executing unchanged steps | Enable `CacheManager` to skip unchanged steps |
| **Can't figure out what went wrong** | Lack of visibility | Check Tracker lineage, generate HTML story |
| **Node keeps failing** | Permanent error | Check `retry_count`, examine error in Tracker, add error hook |
| **Out of memory** | Dataset too large for Pandas | Switch to `SparkEngineContext`, use distributed execution |
| **Secrets exposed in logs** | Hardcoded credentials | Use `context.get_secret()`, never print secrets |
| **Data quality issues** | No validation | Add data quality checks in transform steps or hooks |
| **Pipeline too slow** | Sequential execution | Enable distributed execution with `THREAD_POOL` or `PROCESS_POOL` |
| **Checkpoint not resuming** | Wrong checkpoint mode | Use `CheckpointMode.AUTO` or `LAYER`, check `checkpoint_dir` |
| **Cache not hitting** | Input data changed | Cache invalidates on data changes - expected behavior |

---

## Key Takeaways

âœ… You know the complete pipeline architecture  
âœ… You have a production best practices checklist  
âœ… You understand common pipeline patterns  
âœ… You have 4 complete production-ready examples  
âœ… You know how to troubleshoot common issues  
âœ… You can build production pipelines using ALL odibi_core features

---

## Congratulations! ğŸ‰

You've completed the odibi_core mastery guide! You now have the skills to:

- Build pipelines with 5 node types
- Execute on Pandas or Spark
- Use checkpoints, cache, and retry for reliability
- Monitor with Tracker, Metrics, Events, and Stories
- Integrate with databases and cloud storage
- Schedule and stream data
- Distribute execution for performance
- Create custom transform functions

**Next Steps:**
- Build your own production pipeline
- Explore the [Contract Cheatsheet](ODIBI_CORE_CONTRACT_CHEATSHEET.md) for API details
- Share your pipelines and patterns with the team

---

[â† Back to Index](ODIBI_CORE_MASTERY_INDEX.md)
