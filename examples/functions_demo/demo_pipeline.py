"""
Functions Demo Pipeline - Showcase ODIBI CORE utilities.

Demonstrates the use of general-purpose functions across both
Pandas and Spark engines, including data cleaning, transformation,
validation, and parity checking.
"""

import sys
from pathlib import Path

# Add odibi_core to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from odibi_core.functions import (
    data_ops,
    math_utils,
    string_utils,
    datetime_utils,
    validation_utils,
    conversion_utils,
    helpers
)


def run_pandas_pipeline():
    """Run demo pipeline with Pandas engine."""
    import pandas as pd
    
    print("üêº PANDAS PIPELINE")
    print("=" * 60)
    
    # 1. Load data
    data_dir = Path(__file__).parent
    df = pd.read_csv(data_dir / "messy_data.csv")
    print(f"\n1. Loaded messy data: {len(df)} rows, {len(df.columns)} columns")
    
    # 2. Data quality report
    print("\n2. Initial data quality:")
    report = validation_utils.generate_data_quality_report(df)
    print(f"   - Missing data in Notes: {report['missing_data']['Notes']['count']} rows")
    print(f"   - Duplicate count: {report['duplicate_count']}")
    
    # 3. Clean strings
    print("\n3. Cleaning strings...")
    df = string_utils.standardize_column_names(df, style="snake_case")
    df = string_utils.trim_whitespace(df, "product_name")
    df = string_utils.to_lowercase(df, "product_name")
    print(f"   ‚úì Standardized column names: {list(df.columns)}")
    
    # 4. Convert types
    print("\n4. Converting types...")
    df = conversion_utils.to_boolean(df, "status_flag")
    df = datetime_utils.to_datetime(df, "created_date", format="mixed")
    df = conversion_utils.to_numeric(df, "value", errors="coerce")
    print(f"   ‚úì Converted status_flag to boolean")
    print(f"   ‚úì Parsed created_date to datetime")
    print(f"   ‚úì Converted value to numeric (invalid ‚Üí NaN)")
    
    # 5. Handle nulls
    print("\n5. Handling null values...")
    df = conversion_utils.normalize_nulls(df, "notes")
    df = conversion_utils.fill_null(df, "value", fill_value=0)
    print(f"   ‚úì Normalized null representations")
    print(f"   ‚úì Filled missing values with 0")
    
    # 6. Add date features
    print("\n6. Extracting date features...")
    df = datetime_utils.extract_date_parts(df, "created_date", parts=["year", "month", "day"])
    print(f"   ‚úì Extracted year, month, day")
    
    # 7. Calculate metrics
    print("\n7. Calculating metrics...")
    df = math_utils.calculate_z_score(df, "value")
    df = math_utils.normalize_min_max(df, "value", result_col="value_norm")
    print(f"   ‚úì Calculated z-scores")
    print(f"   ‚úì Min-max normalized values")
    
    # 8. Add metadata
    print("\n8. Adding metadata...")
    df = helpers.add_metadata_columns(df, run_id="demo_run_001", source="messy_bronze")
    print(f"   ‚úì Added run_id and timestamp")
    
    # 9. Validation
    print("\n9. Final validation:")
    missing = validation_utils.check_missing_data(df)
    missing_count = sum(m["count"] for m in missing.values())
    print(f"   - Total missing values: {missing_count}")
    print(f"   - All required columns present: {helpers.ensure_columns_exist(df, ['id', 'product_name', 'value'], raise_error=False)}")
    
    # 10. Sample output
    print("\n10. Sample output:")
    sample = helpers.collect_sample(df, n=3)
    print(sample[["id", "product_name", "status_flag", "value", "value_zscore"]].to_string(index=False))
    
    print(f"\n‚úÖ PANDAS PIPELINE COMPLETE")
    print(f"   Final: {len(df)} rows, {len(df.columns)} columns")
    
    return df


def run_spark_pipeline():
    """Run demo pipeline with Spark engine."""
    try:
        from pyspark.sql import SparkSession
    except ImportError:
        print("\n‚ö†Ô∏è  SPARK NOT AVAILABLE")
        print("   Skipping Spark pipeline (Spark not installed)")
        return None
    
    print("\n\n‚ö° SPARK PIPELINE")
    print("=" * 60)
    
    # Initialize Spark
    spark = SparkSession.builder \
        .appName("FunctionsDemo") \
        .master("local[2]") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    
    # 1. Load data
    data_dir = Path(__file__).parent
    df = spark.read.csv(str(data_dir / "messy_data.csv"), header=True, inferSchema=True)
    print(f"\n1. Loaded messy data: {df.count()} rows, {len(df.columns)} columns")
    
    # 2. Data quality report
    print("\n2. Initial data quality:")
    report = validation_utils.generate_data_quality_report(df)
    print(f"   - Missing data in Notes: {report['missing_data']['Notes']['count']} rows")
    print(f"   - Duplicate count: {report['duplicate_count']}")
    
    # 3. Clean strings
    print("\n3. Cleaning strings...")
    df = string_utils.standardize_column_names(df, style="snake_case")
    df = string_utils.trim_whitespace(df, "product_name")
    df = string_utils.to_lowercase(df, "product_name")
    print(f"   ‚úì Standardized column names: {df.columns}")
    
    # 4. Convert types
    print("\n4. Converting types...")
    df = conversion_utils.to_boolean(df, "status_flag")
    df = datetime_utils.to_datetime(df, "created_date", format=None)
    df = conversion_utils.to_numeric(df, "value", errors="coerce")
    print(f"   ‚úì Converted status_flag to boolean")
    print(f"   ‚úì Parsed created_date to datetime")
    print(f"   ‚úì Converted value to numeric")
    
    # 5. Handle nulls
    print("\n5. Handling null values...")
    df = conversion_utils.normalize_nulls(df, "notes")
    df = conversion_utils.fill_null(df, "value", fill_value=0)
    print(f"   ‚úì Normalized null representations")
    print(f"   ‚úì Filled missing values with 0")
    
    # 6. Add date features
    print("\n6. Extracting date features...")
    df = datetime_utils.extract_date_parts(df, "created_date", parts=["year", "month", "day"])
    print(f"   ‚úì Extracted year, month, day")
    
    # 7. Calculate metrics
    print("\n7. Calculating metrics...")
    df = math_utils.calculate_z_score(df, "value")
    df = math_utils.normalize_min_max(df, "value", result_col="value_norm")
    print(f"   ‚úì Calculated z-scores")
    print(f"   ‚úì Min-max normalized values")
    
    # 8. Add metadata
    print("\n8. Adding metadata...")
    df = helpers.add_metadata_columns(df, run_id="demo_run_001", source="messy_bronze")
    print(f"   ‚úì Added run_id and timestamp")
    
    # 9. Validation
    print("\n9. Final validation:")
    missing = validation_utils.check_missing_data(df)
    missing_count = sum(m["count"] for m in missing.values())
    print(f"   - Total missing values: {missing_count}")
    print(f"   - All required columns present: {helpers.ensure_columns_exist(df, ['id', 'product_name', 'value'], raise_error=False)}")
    
    # 10. Sample output
    print("\n10. Sample output:")
    sample_pd = df.limit(3).toPandas()
    print(sample_pd[["id", "product_name", "status_flag", "value", "value_zscore"]].to_string(index=False))
    
    print(f"\n‚úÖ SPARK PIPELINE COMPLETE")
    print(f"   Final: {df.count()} rows, {len(df.columns)} columns")
    
    spark.stop()
    return df


def compare_engines():
    """Run both pipelines and compare results."""
    print("\n" + "=" * 60)
    print("PARITY VERIFICATION")
    print("=" * 60)
    
    # Note: Full parity comparison would require collecting Spark results
    # For demo purposes, we just show both ran successfully
    print("\n‚úÖ Both Pandas and Spark pipelines completed successfully!")
    print("   Functions work consistently across both engines.")
    print("\nüí° In production, use helpers.compare_results() for detailed parity checks")


def main():
    """Run the complete demo."""
    print("\n" + "=" * 60)
    print("ODIBI CORE FUNCTIONS MODULE - DEMO PIPELINE")
    print("=" * 60)
    print("\nThis demo showcases general-purpose utilities:")
    print("  ‚Ä¢ Data cleaning (strings, nulls)")
    print("  ‚Ä¢ Type conversion (boolean, numeric, datetime)")
    print("  ‚Ä¢ Mathematical operations (z-scores, normalization)")
    print("  ‚Ä¢ Validation (quality checks)")
    print("  ‚Ä¢ Metadata tracking")
    print("  ‚Ä¢ Engine parity (Pandas ‚ü∑ Spark)")
    
    # Run pipelines
    df_pandas = run_pandas_pipeline()
    df_spark = run_spark_pipeline()
    
    # Compare
    if df_spark is not None:
        compare_engines()
    
    print("\n" + "=" * 60)
    print("DEMO COMPLETE ‚úÖ")
    print("=" * 60)


if __name__ == "__main__":
    main()
