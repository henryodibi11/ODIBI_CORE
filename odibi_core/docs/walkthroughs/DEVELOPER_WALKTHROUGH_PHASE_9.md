---
id: phase_9_sdk
title: "Phase 9: SDK & CLI Design"
level: "Advanced"
tags: ["sdk", "cli", "api-design"]
checkpoints:
  - id: checkpoint_1
    title: "SDK Module Structure & Version Tracking"
    description: "Create SDK module structure, version module, and framework manifest"
  - id: checkpoint_2
    title: "Configuration Validator Foundation"
    description: "Build validation data structures and core validator class"
  - id: checkpoint_3
    title: "Step Validation & Circular Dependency Detection"
    description: "Implement step validation logic and circular dependency checker"
  - id: checkpoint_4
    title: "SDK Layer - ODIBI Class"
    description: "Create static ODIBI class with run(), validate(), version() methods"
  - id: checkpoint_5
    title: "SDK Layer - Pipeline Class"
    description: "Build Pipeline class with fluent API and alternate constructors"
  - id: checkpoint_6
    title: "CLI Tool - Command Structure"
    description: "Implement CLI with Click, create run/validate/version commands"
  - id: checkpoint_7
    title: "CLI Tool - Rich Output Formatting"
    description: "Add colorized, formatted output using Rich library"
  - id: checkpoint_8
    title: "Package Configuration & Entry Points"
    description: "Update pyproject.toml with CLI entry point and dependencies"
  - id: checkpoint_9
    title: "Integration Testing & Verification"
    description: "Run verification script and ensure all components work together"
quiz_questions:
  - id: q1
    checkpoint: checkpoint_1
    question: "What is the primary purpose of the `__version__.py` file?"
    options:
      - "Store database credentials"
      - "Single source of truth for version information"
      - "Define API routes"
      - "Configure logging"
    correct: 1
  - id: q2
    checkpoint: checkpoint_1
    question: "What information does `manifest.json` provide?"
    options:
      - "User credentials"
      - "Database schemas"
      - "Framework capabilities and metadata"
      - "API endpoints"
    correct: 2
  - id: q3
    checkpoint: checkpoint_1
    question: "Which two engines are listed in `__supported_engines__`?"
    options:
      - "mysql and postgres"
      - "pandas and spark"
      - "redis and mongodb"
      - "hadoop and hive"
    correct: 1
  - id: q4
    checkpoint: checkpoint_2
    question: "Why use an Enum for `ValidationLevel`?"
    options:
      - "It's faster than strings"
      - "Prevents typos and provides IDE autocomplete"
      - "Required by Python"
      - "Uses less memory"
    correct: 1
  - id: q5
    checkpoint: checkpoint_2
    question: "What are the three validation severity levels?"
    options:
      - "low, medium, high"
      - "error, warning, info"
      - "critical, major, minor"
      - "fail, warn, pass"
    correct: 1
  - id: q6
    checkpoint: checkpoint_2
    question: "What is the purpose of the `strict` parameter in `validate()`?"
    options:
      - "Enforce password complexity"
      - "Make warnings also fail validation"
      - "Enable debug mode"
      - "Skip validation checks"
    correct: 1
  - id: q7
    checkpoint: checkpoint_3
    question: "What are the three required fields for every step?"
    options:
      - "id, type, engine"
      - "layer, name, type"
      - "name, value, output"
      - "config, input, output"
    correct: 1
  - id: q8
    checkpoint: checkpoint_3
    question: "How does circular dependency detection work?"
    options:
      - "Random sampling"
      - "DFS traversal with visited/path sets"
      - "Breadth-first search only"
      - "Checking step names"
    correct: 1
  - id: q9
    checkpoint: checkpoint_3
    question: "What validation level is used for unknown engines?"
    options:
      - "ERROR (blocks execution)"
      - "WARNING (allows execution)"
      - "INFO (just logs)"
      - "CRITICAL (crashes)"
    correct: 1
  - id: q10
    checkpoint: checkpoint_4
    question: "Why use @staticmethod for ODIBI.run()?"
    options:
      - "It's required for CLI tools"
      - "Follows familiar Python pattern (requests.get, json.dumps)"
      - "Improves performance"
      - "Enables inheritance"
    correct: 1
  - id: q11
    checkpoint: checkpoint_4
    question: "What does ODIBI.run() return?"
    options:
      - "Boolean success/failure"
      - "PipelineResult dataclass"
      - "Dictionary of outputs"
      - "List of errors"
    correct: 1
  - id: q12
    checkpoint: checkpoint_4
    question: "What is the main benefit of the SDK wrapper pattern?"
    options:
      - "Hides all implementation details"
      - "Simplifies common cases without limiting advanced users"
      - "Replaces core modules completely"
      - "Forces a specific coding style"
    correct: 1
  - id: q13
    checkpoint: checkpoint_5
    question: "What makes the Pipeline API 'fluent'?"
    options:
      - "It uses lambda functions"
      - "Methods return self enabling chaining"
      - "It's written in multiple languages"
      - "It auto-completes code"
    correct: 1
  - id: q14
    checkpoint: checkpoint_5
    question: "What is an alternate constructor?"
    options:
      - "A backup constructor if the first fails"
      - "A @classmethod providing different ways to create objects"
      - "A deprecated constructor"
      - "A constructor for subclasses only"
    correct: 1
  - id: q15
    checkpoint: checkpoint_5
    question: "What does `Pipeline.from_config()` return?"
    options:
      - "Dictionary of steps"
      - "ConfigLoader object"
      - "Pipeline instance"
      - "JSON string"
    correct: 2
  - id: q16
    checkpoint: checkpoint_6
    question: "Which library is used to build the CLI?"
    options:
      - "argparse"
      - "Click"
      - "optparse"
      - "docopt"
    correct: 1
  - id: q17
    checkpoint: checkpoint_6
    question: "What are the three main CLI commands?"
    options:
      - "start, stop, status"
      - "run, validate, version"
      - "install, build, test"
      - "init, exec, clean"
    correct: 1
  - id: q18
    checkpoint: checkpoint_6
    question: "What exit code indicates success in Unix conventions?"
    options:
      - "-1"
      - "0"
      - "1"
      - "200"
    correct: 1
  - id: q19
    checkpoint: checkpoint_7
    question: "Which library provides colorized CLI output?"
    options:
      - "colorama"
      - "termcolor"
      - "Rich"
      - "blessed"
    correct: 2
  - id: q20
    checkpoint: checkpoint_7
    question: "What Rich feature displays progress during execution?"
    options:
      - "Progress bar"
      - "Spinner"
      - "Table"
      - "All of the above"
    correct: 3
  - id: q21
    checkpoint: checkpoint_7
    question: "Why use Rich instead of basic print statements?"
    options:
      - "It's faster"
      - "Better user experience with colors, tables, progress indicators"
      - "Required by Click"
      - "Uses less memory"
    correct: 1
  - id: q22
    checkpoint: checkpoint_8
    question: "Where is the CLI entry point defined?"
    options:
      - "setup.py"
      - "pyproject.toml [project.scripts]"
      - "manifest.json"
      - "CLI.md"
    correct: 1
  - id: q23
    checkpoint: checkpoint_8
    question: "What command makes the package installable?"
    options:
      - "python setup.py install"
      - "pip install ."
      - "python -m install"
      - "odibi install"
    correct: 1
  - id: q24
    checkpoint: checkpoint_8
    question: "What are the new dependencies added for Phase 9?"
    options:
      - "numpy and scipy"
      - "click and rich"
      - "flask and django"
      - "pytest and black"
    correct: 1
  - id: q25
    checkpoint: checkpoint_9
    question: "How many tests does the verification script run?"
    options:
      - "3"
      - "6"
      - "9"
      - "12"
    correct: 1
  - id: q26
    checkpoint: checkpoint_9
    question: "What should happen to Phase 1-8 code after Phase 9?"
    options:
      - "It must be rewritten"
      - "It still works (100% backward compatible)"
      - "It needs minor updates"
      - "It's deprecated"
    correct: 1
  - id: q27
    checkpoint: checkpoint_9
    question: "What is the recommended approach after building Phase 9?"
    options:
      - "Delete old code"
      - "Run verification script and check diagnostics"
      - "Immediately deploy to production"
      - "Start Phase 10 without testing"
    correct: 1
---

# ODIBI CORE v1.0 - Phase 9 Developer Walkthrough

**Building SDK & CLI: A Step-by-Step Productization Guide**

**Author**: AMP AI Engineering Agent  
**Date**: 2025-11-01  
**Audience**: Developers learning SDK design and CLI development  
**Duration**: ~3 hours (following this guide)  
**Prerequisites**: Completed Phase 8 (Observability & Automation)

---

## üìö SDK & Productization Overview

### What is Phase 9?

Phase 9 transforms ODIBI CORE from a **development framework** into a **production-ready, installable SDK** with command-line tools. You'll add:

- **SDK Layer** - Clean, Pythonic API (`ODIBI`, `Pipeline` classes)
- **CLI Tool** - `odibi` command for terminal execution
- **ConfigValidator** - Pre-execution validation with circular dependency detection
- **Version Management** - Centralized version tracking
- **Package Metadata** - Framework capability manifest

### What's New in Phase 9

**Phase 8**: Framework with full observability
```python
# Complex setup required
from odibi_core.core import ConfigLoader, Orchestrator
from odibi_core.engine import PandasEngineContext
from odibi_core.core import Tracker, EventEmitter

loader = ConfigLoader()
steps = loader.load("pipeline.json")
context = PandasEngineContext(secrets={"db_pass": "secret"})
tracker = Tracker()
events = EventEmitter()
orchestrator = Orchestrator(steps, context, tracker, events)
result = orchestrator.run()
```

**Phase 9**: SDK with simple API
```python
# One-line execution
from odibi_core.sdk import ODIBI
result = ODIBI.run("pipeline.json", engine="pandas", secrets={"db_pass": "secret"})
```

**Phase 9**: CLI for terminal use
```bash
# Even simpler - no Python required
odibi run --config pipeline.json --engine pandas
```

### Key Architecture Components

**1. SDK Layer (Developer-Friendly API)**
```python
# Quick execution
result = ODIBI.run("pipeline.json", engine="pandas")

# Advanced configuration
pipeline = Pipeline.from_config("pipeline.json")
pipeline.set_engine("pandas").set_secrets(secrets).set_parallelism(8)
result = pipeline.execute()
```

**2. CLI Tool (Command-Line Interface)**
```bash
odibi run --config pipeline.json --engine pandas --workers 8
odibi validate --config pipeline.json --strict
odibi version
```

**3. ConfigValidator (Pre-Execution Validation)**
```python
validator = ConfigValidator()
is_valid = validator.validate("pipeline.json")
# Checks: required fields, circular deps, valid engines, JSON syntax
```

### What You'll Build

By the end of this walkthrough, you'll have:
- ‚úÖ SDK with `ODIBI` and `Pipeline` classes
- ‚úÖ CLI tool with run, validate, version commands
- ‚úÖ Configuration validator with circular dependency detection
- ‚úÖ Version tracking system (`__version__.py`)
- ‚úÖ Framework metadata (`manifest.json`)
- ‚úÖ Clean `pip install .` packaging
- ‚úÖ Full backward compatibility with Phases 1-8

### Time Investment

- Reading this guide: ~45 minutes
- Building along: ~2.5-3 hours
- Understanding SDK design: Priceless

---

## üó∫Ô∏è Dependency Map (Phase 9)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              User Code (CLI or SDK)                         ‚îÇ
‚îÇ    odibi run --config pipeline.json                         ‚îÇ
‚îÇ    ODIBI.run("pipeline.json")                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñ≤
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CLI (cli.py)   ‚îÇ SDK Layer       ‚îÇ ConfigValidator  ‚îÇ
‚îÇ                  ‚îÇ (sdk/__init__)  ‚îÇ (sdk/validator)  ‚îÇ
‚îÇ                  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ Provides:        ‚îÇ Provides:       ‚îÇ Provides:        ‚îÇ
‚îÇ - odibi run      ‚îÇ - ODIBI.run()   ‚îÇ - Config checks  ‚îÇ
‚îÇ - odibi validate ‚îÇ - Pipeline API  ‚îÇ - Cycle detect   ‚îÇ
‚îÇ - odibi version  ‚îÇ - PipelineResult‚îÇ - Field valid    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñ≤
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Phase 8: Observability (StructuredLogger, EventBus)      ‚îÇ
‚îÇ   Phase 7: Cloud (CloudAdapter, DistributedExecutor)       ‚îÇ
‚îÇ   Phase 5: DAG (DAGExecutor, DAGBuilder)                   ‚îÇ
‚îÇ   Phase 3: Config (ConfigLoader)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Build Order**: Version Module ‚Üí Manifest ‚Üí ConfigValidator ‚Üí SDK Layer ‚Üí CLI ‚Üí Documentation

---

## üéØ Mission-Based Build Plan

### Mission 1: Create SDK Module Structure & Version Tracking (15 mins)

**Goal**: Set up directory structure and version management

#### Step 1.1: Create SDK Module

**Action**:
```bash
cd odibi_core
mkdir sdk
cd sdk
touch __init__.py config_validator.py doc_generator.py
```

**Verification**:
```bash
ls -la odibi_core/sdk/
# Should show: __init__.py, config_validator.py, doc_generator.py
```

#### Step 1.2: Create Version Module

**File**: `odibi_core/__version__.py`

```python
"""Version information for ODIBI CORE."""

from datetime import datetime

__version__ = "1.0.0"
__phase__ = "v1.0-phase9"
__author__ = "Henry Odibi"
__build_date__ = datetime.now().isoformat()
__supported_engines__ = ["pandas", "spark"]
__python_requires__ = ">=3.8"
```

**Why separate version file?**
- Single source of truth for version
- Importable from setup.py and code
- Includes build metadata for debugging
- Phase tracking for roadmap visibility

#### Step 1.3: Update Main __init__.py

**File**: `odibi_core/__init__.py`

**Before**:
```python
__version__ = "1.0.0"
__author__ = "Henry Odibi"
```

**After**:
```python
from odibi_core.__version__ import __version__, __author__, __phase__
```

**Why?** Centralized version management prevents version drift between files.

#### Step 1.4: Create Framework Manifest

**File**: `manifest.json` (in project root)

```json
{
  "name": "odibi-core",
  "version": "1.0.0",
  "phase": "v1.0-phase9",
  "build_timestamp": "2025-11-01T00:00:00",
  "framework": {
    "type": "data-engineering",
    "architecture": "node-centric",
    "paradigm": "config-driven"
  },
  "supported_engines": [
    {
      "name": "pandas",
      "version": ">=1.5.0",
      "runtime": "local",
      "sql_backend": "duckdb"
    },
    {
      "name": "spark",
      "version": ">=3.3.0",
      "runtime": "distributed",
      "sql_backend": "spark-sql"
    }
  ],
  "node_types": [
    "ConnectNode",
    "IngestNode",
    "StoreNode",
    "TransformNode",
    "PublishNode"
  ],
  "features": {
    "dag_execution": true,
    "streaming": true,
    "scheduling": true,
    "distributed": true,
    "cloud_integration": true,
    "observability": true,
    "sdk": true,
    "cli": true
  },
  "cloud_platforms": {
    "azure": ["blob", "adls", "event_hubs"],
    "aws": ["s3", "kinesis"],
    "hadoop": ["hdfs"]
  },
  "observability": {
    "structured_logging": true,
    "metrics_export": ["prometheus", "json", "parquet"],
    "event_bus": true,
    "automation_hooks": true
  },
  "metadata": {
    "author": "Henry Odibi",
    "license": "MIT",
    "python_requires": ">=3.8",
    "amp_generated": true
  }
}
```

**Why manifest.json?**
- Machine-readable capability discovery
- Version tracking for integrations
- Feature flags for conditional behavior
- Documentation for users about what's included

#### Verification

**Test version import**:
```python
from odibi_core.__version__ import __version__, __phase__, __supported_engines__

print(f"Version: {__version__}")  # Should print: 1.0.0
print(f"Phase: {__phase__}")      # Should print: v1.0-phase9
print(f"Engines: {__supported_engines__}")  # Should print: ['pandas', 'spark']
```

**Test manifest loading**:
```python
import json
from pathlib import Path

manifest_path = Path("manifest.json")
with open(manifest_path, "r") as f:
    manifest = json.load(f)

print(f"Framework: {manifest['name']} v{manifest['version']}")
print(f"SDK enabled: {manifest['features']['sdk']}")
```

---

### Mission 2: Build Configuration Validator (45 mins)

**Goal**: Implement pre-execution validation with circular dependency detection

#### Step 2.1: Define Validation Data Structures

**File**: `odibi_core/sdk/config_validator.py`

```python[demo]
"""
Configuration validator for pipeline definitions.

Validates JSON, SQLite, and CSV configurations against required schema.
"""

import json
import sqlite3
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Set
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class ValidationLevel(str, Enum):
    """Validation severity levels."""
    ERROR = "error"
    WARNING = "warning"
    INFO = "info"


@dataclass
class ValidationIssue:
    """
    Represents a validation issue.
    
    Attributes:
        level: Severity level
        message: Issue description
        step_name: Name of the step (if applicable)
        field: Field name (if applicable)
    """
    level: ValidationLevel
    message: str
    step_name: Optional[str] = None
    field: Optional[str] = None
    
    def __str__(self) -> str:
        prefix = "‚ùå" if self.level == ValidationLevel.ERROR else "‚ö†Ô∏è"
        step_info = f" [{self.step_name}]" if self.step_name else ""
        field_info = f" (field: {self.field})" if self.field else ""
        return f"{prefix} {self.message}{step_info}{field_info}"
```

**Why dataclass?** Type safety, automatic equality checks, clean string representation.

**Why enum for levels?** Prevents typos, provides IDE autocomplete, enables filtering.

#### Step 2.2: Implement Core Validator Class

**Add to same file**:

```python[demo]
class ConfigValidator:
    """
    Validate pipeline configuration files.
    
    Checks:
    - Required fields (layer, name, type)
    - Valid layers (connect, ingest, store, transform, publish)
    - Valid engines (pandas, spark)
    - Duplicate step names
    - Circular dependencies
    - JSON syntax in inputs/outputs
    
    Example:
        >>> validator = ConfigValidator()
        >>> result = validator.validate("pipeline.json")
        >>> if not result:
        ...     for issue in validator.issues:
        ...         print(issue)
    """
    
    REQUIRED_FIELDS = {"layer", "name", "type"}
    VALID_LAYERS = {"connect", "ingest", "store", "transform", "publish"}
    VALID_ENGINES = {"pandas", "spark"}
    
    def __init__(self) -> None:
        self.issues: List[ValidationIssue] = []
        self.step_names: Set[str] = set()
    
    def validate(self, config_path: str, strict: bool = False) -> bool:
        """
        Validate configuration file.
        
        Args:
            config_path: Path to configuration file
            strict: If True, warnings also fail validation
        
        Returns:
            True if valid, False otherwise
        
        Raises:
            FileNotFoundError: If config file doesn't exist
        """
        self.issues = []
        self.step_names = set()
        
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        # Load config based on type
        if config_file.suffix == ".json":
            steps = self._load_json(config_file)
        elif config_file.suffix == ".db":
            steps = self._load_sqlite(config_file)
        elif config_file.suffix == ".csv":
            steps = self._load_csv(config_file)
        else:
            self.issues.append(
                ValidationIssue(
                    level=ValidationLevel.ERROR,
                    message=f"Unsupported file format: {config_file.suffix}",
                )
            )
            return False
        
        if not steps:
            self.issues.append(
                ValidationIssue(
                    level=ValidationLevel.ERROR,
                    message="No steps found in config"
                )
            )
            return False
        
        # Validate each step
        for step in steps:
            self._validate_step(step)
        
        # Check for circular dependencies
        self._check_circular_dependencies(steps)
        
        # Check for errors
        has_errors = any(issue.level == ValidationLevel.ERROR for issue in self.issues)
        has_warnings = any(issue.level == ValidationLevel.WARNING for issue in self.issues)
        
        if strict and has_warnings:
            return False
        
        return not has_errors
```

**Why separate load methods?** Different file formats have different parsing logic. Separation enables easy addition of new formats.

**Why strict mode?** Production environments may want to fail on warnings (e.g., unknown engines).

#### Step 2.3: Implement Step Validation

**Add to ConfigValidator class**:

```python[demo]
def _validate_step(self, step: Dict[str, Any]) -> None:
    """Validate a single step configuration."""
    step_name = step.get("name", "<unnamed>")
    
    # Check required fields
    for field in self.REQUIRED_FIELDS:
        if field not in step:
            self.issues.append(
                ValidationIssue(
                    level=ValidationLevel.ERROR,
                    message=f"Missing required field: {field}",
                    step_name=step_name,
                    field=field,
                )
            )
    
    # Check layer validity
    layer = step.get("layer", "")
    if layer and layer not in self.VALID_LAYERS:
        self.issues.append(
            ValidationIssue(
                level=ValidationLevel.ERROR,
                message=f"Invalid layer: {layer}. Must be one of {self.VALID_LAYERS}",
                step_name=step_name,
                field="layer",
            )
        )
    
    # Check engine validity (warning, not error - future engines may be added)
    engine = step.get("engine", "")
    if engine and engine not in self.VALID_ENGINES:
        self.issues.append(
            ValidationIssue(
                level=ValidationLevel.WARNING,
                message=f"Unknown engine: {engine}. Expected: {self.VALID_ENGINES}",
                step_name=step_name,
                field="engine",
            )
        )
    
    # Check for duplicate step names
    if step_name in self.step_names:
        self.issues.append(
            ValidationIssue(
                level=ValidationLevel.ERROR,
                message=f"Duplicate step name: {step_name}",
                step_name=step_name,
            )
        )
    else:
        self.step_names.add(step_name)
```

**Why warnings for unknown engines?** Future-proofing - new engines may be added without updating validator.

**Why track step_names in a set?** O(1) duplicate detection, efficient for large configs.

#### Step 2.4: Implement Circular Dependency Detection

**Add to ConfigValidator class**:

```python[demo]
def _check_circular_dependencies(self, steps: List[Dict[str, Any]]) -> None:
    """Check for circular dependencies using DFS."""
    # Build dependency graph
    graph: Dict[str, List[str]] = {}
    for step in steps:
        step_name = step.get("name", "")
        inputs = step.get("inputs", {})
        
        # Extract upstream dependencies from inputs
        dependencies = []
        if isinstance(inputs, dict):
            for value in inputs.values():
                if isinstance(value, str):
                    dependencies.append(value.split(".")[0])  # Handle "step.output" format
        
        graph[step_name] = dependencies
    
    # DFS to detect cycles
    visited: Set[str] = set()
    path: Set[str] = set()
    
    def dfs(node: str) -> bool:
        """Returns True if cycle detected."""
        if node in path:
            return True  # Cycle found
        if node in visited:
            return False  # Already checked
        
        visited.add(node)
        path.add(node)
        
        for neighbor in graph.get(node, []):
            if neighbor in graph and dfs(neighbor):
                self.issues.append(
                    ValidationIssue(
                        level=ValidationLevel.ERROR,
                        message=f"Circular dependency detected involving: {node} ‚Üí {neighbor}",
                        step_name=node,
                    )
                )
                return True
        
        path.remove(node)
        return False
    
    for step_name in graph:
        if step_name not in visited:
            dfs(step_name)
```

**Why DFS?** Efficient cycle detection with O(V+E) complexity, maintains path for reporting circular chains.

**Why separate visited and path sets?** `visited` tracks completed nodes, `path` tracks current recursion stack for cycle detection.

#### Step 2.5: Implement Config Loaders

**Add to ConfigValidator class**:

```python[demo]
def _load_json(self, config_file: Path) -> List[Dict[str, Any]]:
    """Load configuration from JSON file."""
    try:
        with open(config_file, "r") as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return data
        elif isinstance(data, dict) and "steps" in data:
            return data["steps"]
        else:
            self.issues.append(
                ValidationIssue(
                    level=ValidationLevel.ERROR,
                    message="JSON must be a list of steps or dict with 'steps' key"
                )
            )
            return []
    except json.JSONDecodeError as e:
        self.issues.append(
            ValidationIssue(
                level=ValidationLevel.ERROR,
                message=f"Invalid JSON syntax: {e}"
            )
        )
        return []

def _load_sqlite(self, config_file: Path) -> List[Dict[str, Any]]:
    """Load configuration from SQLite database."""
    try:
        conn = sqlite3.connect(str(config_file))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM pipeline_config")
        rows = cursor.fetchall()
        conn.close()
        
        return [dict(row) for row in rows]
    except sqlite3.Error as e:
        self.issues.append(
            ValidationIssue(
                level=ValidationLevel.ERROR,
                message=f"SQLite error: {e}"
            )
        )
        return []

def _load_csv(self, config_file: Path) -> List[Dict[str, Any]]:
    """Load configuration from CSV file."""
    try:
        import csv
        with open(config_file, "r") as f:
            reader = csv.DictReader(f)
            return list(reader)
    except Exception as e:
        self.issues.append(
            ValidationIssue(
                level=ValidationLevel.ERROR,
                message=f"CSV error: {e}"
            )
        )
        return []
```

**Why support multiple formats?** ODIBI CORE allows JSON, SQLite, CSV configs - validator must handle all.

**Why catch specific exceptions?** Provides clear error messages for debugging, separates validation logic from I/O errors.

---

### Mission 3: Build SDK Layer - ODIBI Class (30 mins)

**Goal**: Create static utility class for simple pipeline execution

#### Step 3.1: Create SDK __init__.py with ODIBI Class

**File**: `odibi_core/sdk/__init__.py`

```python[demo]
"""
ODIBI CORE SDK - Developer-friendly API for pipeline execution.

Example:
    >>> from odibi_core.sdk import ODIBI
    >>> result = ODIBI.run("pipeline.json", engine="pandas")
    >>> print(result.is_success())
"""

import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path

from odibi_core.core import ConfigLoader, Orchestrator
from odibi_core.engine import PandasEngineContext, SparkEngineContext
from odibi_core.observability import Tracker, EventEmitter
from odibi_core.sdk.config_validator import ConfigValidator
from odibi_core.__version__ import __version__, __phase__

logger = logging.getLogger(__name__)


@dataclass
class PipelineResult:
    """
    Result of pipeline execution.
    
    Attributes:
        success_count: Number of successful nodes
        failed_count: Number of failed nodes
        total_duration_ms: Total execution time in milliseconds
        outputs: Dictionary of output artifacts
    """
    success_count: int
    failed_count: int
    total_duration_ms: float
    outputs: Dict[str, Any]
    
    def is_success(self) -> bool:
        """Check if pipeline executed successfully."""
        return self.failed_count == 0


class ODIBI:
    """
    Static utility class for ODIBI pipeline execution.
    
    Provides simple, one-line API for running pipelines.
    
    Example:
        >>> result = ODIBI.run("pipeline.json", engine="pandas")
        >>> if result.is_success():
        ...     print(f"Pipeline completed in {result.total_duration_ms}ms")
    """
    
    @staticmethod
    def run(
        config_path: str,
        engine: str = "pandas",
        secrets: Optional[Dict[str, Any]] = None,
        validate: bool = True,
        verbose: bool = False,
    ) -> PipelineResult:
        """
        Execute a pipeline from configuration file.
        
        Args:
            config_path: Path to pipeline configuration (JSON, SQLite, CSV)
            engine: Execution engine ("pandas" or "spark")
            secrets: Dictionary of secrets for nodes
            validate: Whether to validate config before execution
            verbose: Enable verbose logging
        
        Returns:
            PipelineResult with execution statistics
        
        Raises:
            ValueError: If validation fails or config is invalid
            FileNotFoundError: If config file not found
        """
        if verbose:
            logging.basicConfig(level=logging.DEBUG)
        
        # Validate config if requested
        if validate:
            validator = ConfigValidator()
            if not validator.validate(config_path):
                error_messages = [str(issue) for issue in validator.issues]
                raise ValueError(f"Config validation failed:\n" + "\n".join(error_messages))
        
        # Load configuration
        loader = ConfigLoader()
        steps = loader.load(config_path)
        
        # Create engine context
        if engine == "pandas":
            context = PandasEngineContext(secrets=secrets or {})
        elif engine == "spark":
            context = SparkEngineContext(secrets=secrets or {})
        else:
            raise ValueError(f"Unknown engine: {engine}. Must be 'pandas' or 'spark'")
        
        # Create observability components
        tracker = Tracker()
        emitter = EventEmitter()
        
        # Execute pipeline
        orchestrator = Orchestrator(steps, context, tracker, emitter)
        result = orchestrator.run()
        
        # Package result
        return PipelineResult(
            success_count=result.get("success_count", 0),
            failed_count=result.get("failed_count", 0),
            total_duration_ms=result.get("total_duration_ms", 0.0),
            outputs=result.get("outputs", {}),
        )
    
    @staticmethod
    def validate(config_path: str, strict: bool = False) -> bool:
        """
        Validate a pipeline configuration without executing.
        
        Args:
            config_path: Path to configuration file
            strict: If True, warnings also fail validation
        
        Returns:
            True if valid, False otherwise
        """
        validator = ConfigValidator()
        is_valid = validator.validate(config_path, strict=strict)
        
        # Print issues
        for issue in validator.issues:
            print(issue)
        
        return is_valid
    
    @staticmethod
    def version() -> str:
        """Get ODIBI CORE version."""
        return __version__
    
    @staticmethod
    def info() -> Dict[str, Any]:
        """Get framework information."""
        return {
            "version": __version__,
            "phase": __phase__,
            "supported_engines": ["pandas", "spark"],
        }
```

**Why @staticmethod?** No instance needed, follows Python conventions (like `json.dumps()`, `requests.get()`).

**Why PipelineResult dataclass?** Type-safe result handling, clean API, easy to extend.

---

### Mission 4: Build SDK Layer - Pipeline Class (30 mins)

**Goal**: Create fluent API for advanced pipeline configuration

#### Step 4.1: Add Pipeline Class to SDK

**Add to `odibi_core/sdk/__init__.py`**:

```python[demo]
class Pipeline:
    """
    Fluent API for pipeline configuration and execution.
    
    Supports method chaining for readable configuration:
        >>> pipeline = (Pipeline.from_config("config.json")
        ...             .set_engine("pandas")
        ...             .set_secrets(secrets)
        ...             .set_parallelism(8))
        >>> result = pipeline.execute()
    """
    
    def __init__(self, steps: list, name: str = "unnamed_pipeline"):
        """
        Create pipeline from steps list.
        
        Args:
            steps: List of Step objects or dictionaries
            name: Pipeline name for logging
        """
        self.steps = steps
        self.name = name
        self.engine = "pandas"
        self.secrets: Dict[str, Any] = {}
        self.parallelism = 1
        self.validate_config = True
        self.verbose = False
    
    @classmethod
    def from_config(cls, config_path: str, name: Optional[str] = None) -> "Pipeline":
        """
        Create pipeline from configuration file.
        
        Args:
            config_path: Path to configuration file
            name: Optional pipeline name
        
        Returns:
            Pipeline instance
        """
        loader = ConfigLoader()
        steps = loader.load(config_path)
        
        if name is None:
            name = Path(config_path).stem
        
        return cls(steps, name=name)
    
    def set_engine(self, engine: str) -> "Pipeline":
        """Set execution engine (pandas or spark)."""
        self.engine = engine
        return self
    
    def set_secrets(self, secrets: Dict[str, Any]) -> "Pipeline":
        """Set secrets for node execution."""
        self.secrets = secrets
        return self
    
    def set_parallelism(self, workers: int) -> "Pipeline":
        """Set parallel execution workers."""
        self.parallelism = workers
        return self
    
    def set_validation(self, validate: bool) -> "Pipeline":
        """Enable/disable pre-execution validation."""
        self.validate_config = validate
        return self
    
    def set_verbose(self, verbose: bool) -> "Pipeline":
        """Enable/disable verbose logging."""
        self.verbose = verbose
        return self
    
    def execute(self) -> PipelineResult:
        """
        Execute the pipeline.
        
        Returns:
            PipelineResult with execution statistics
        """
        # Use ODIBI.run() internally
        # (In real implementation, would build orchestrator directly)
        logger.info(f"Executing pipeline: {self.name}")
        
        # Create engine context
        if self.engine == "pandas":
            context = PandasEngineContext(secrets=self.secrets)
        elif self.engine == "spark":
            context = SparkEngineContext(secrets=self.secrets)
        else:
            raise ValueError(f"Unknown engine: {self.engine}")
        
        # Create observability
        tracker = Tracker()
        emitter = EventEmitter()
        
        # Execute
        orchestrator = Orchestrator(self.steps, context, tracker, emitter)
        result = orchestrator.run()
        
        return PipelineResult(
            success_count=result.get("success_count", 0),
            failed_count=result.get("failed_count", 0),
            total_duration_ms=result.get("total_duration_ms", 0.0),
            outputs=result.get("outputs", {}),
        )


# Export public API
__all__ = ["ODIBI", "Pipeline", "PipelineResult", "ConfigValidator"]
```

**Why method chaining?** Improves readability, reduces variable declarations, follows jQuery/pandas patterns.

**Why @classmethod for from_config?** Alternate constructor pattern - provides flexibility while maintaining type safety.

---

### Mission 5: Build CLI Tool (45 mins)

**Goal**: Create command-line interface using Click and Rich

#### Step 5.1: Install Dependencies

**Update `pyproject.toml`**:

```toml
[project]
dependencies = [
    "pandas>=1.5.0",
    "duckdb>=0.8.0",
    "click>=8.1.0",
    "rich>=13.0.0",
]

[project.scripts]
odibi = "odibi_core.cli:main"
```

**Install**:
```bash
pip install click rich
```

#### Step 5.2: Create CLI Module

**File**: `odibi_core/cli.py`

```python[demo]
"""
ODIBI CORE Command-Line Interface.

Usage:
    odibi run --config pipeline.json --engine pandas
    odibi validate --config pipeline.json --strict
    odibi version
"""

import sys
import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from pathlib import Path

from odibi_core.sdk import ODIBI, ConfigValidator
from odibi_core.__version__ import __version__, __phase__

console = Console()


@click.group()
@click.version_option(version=__version__, prog_name="ODIBI CORE")
def main():
    """ODIBI CORE - Config-Driven Data Engineering Framework."""
    pass


@main.command()
@click.option("--config", "-c", required=True, help="Path to pipeline configuration file")
@click.option("--engine", "-e", default="pandas", help="Execution engine (pandas or spark)")
@click.option("--secrets-file", "-s", help="Path to secrets JSON file")
@click.option("--validate/--no-validate", default=True, help="Validate config before execution")
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose logging")
def run(config, engine, secrets_file, validate, verbose):
    """Execute a pipeline from configuration file."""
    console.print(f"[bold blue]ODIBI CORE v{__version__}[/bold blue]")
    console.print(f"Executing: {config}")
    
    # Load secrets if provided
    secrets = {}
    if secrets_file:
        import json
        with open(secrets_file, "r") as f:
            secrets = json.load(f)
    
    try:
        # Execute with progress indicator
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("Running pipeline...", total=None)
            
            result = ODIBI.run(
                config_path=config,
                engine=engine,
                secrets=secrets,
                validate=validate,
                verbose=verbose,
            )
            
            progress.update(task, completed=True)
        
        # Display results
        if result.is_success():
            console.print(f"[bold green]‚úÖ Pipeline succeeded[/bold green]")
            console.print(f"Nodes executed: {result.success_count}")
            console.print(f"Duration: {result.total_duration_ms:.2f}ms")
            sys.exit(0)
        else:
            console.print(f"[bold red]‚ùå Pipeline failed[/bold red]")
            console.print(f"Failed nodes: {result.failed_count}")
            sys.exit(1)
    
    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {e}")
        if verbose:
            console.print_exception()
        sys.exit(1)


@main.command()
@click.option("--config", "-c", required=True, help="Path to pipeline configuration file")
@click.option("--strict", is_flag=True, help="Treat warnings as errors")
def validate(config, strict):
    """Validate a pipeline configuration without executing."""
    console.print(f"[bold blue]Validating: {config}[/bold blue]")
    
    validator = ConfigValidator()
    is_valid = validator.validate(config, strict=strict)
    
    # Display issues
    if validator.issues:
        table = Table(title="Validation Issues")
        table.add_column("Level", style="cyan")
        table.add_column("Step", style="magenta")
        table.add_column("Message", style="white")
        
        for issue in validator.issues:
            level_color = "red" if issue.level.value == "error" else "yellow"
            table.add_row(
                f"[{level_color}]{issue.level.value.upper()}[/{level_color}]",
                issue.step_name or "-",
                issue.message,
            )
        
        console.print(table)
    
    if is_valid:
        console.print("[bold green]‚úÖ Configuration is valid[/bold green]")
        sys.exit(0)
    else:
        console.print("[bold red]‚ùå Configuration is invalid[/bold red]")
        sys.exit(1)


@main.command()
def version():
    """Display version information."""
    info = ODIBI.info()
    
    table = Table(title="ODIBI CORE")
    table.add_column("Property", style="cyan")
    table.add_column("Value", style="green")
    
    table.add_row("Version", info["version"])
    table.add_row("Phase", info["phase"])
    table.add_row("Engines", ", ".join(info["supported_engines"]))
    
    console.print(table)


if __name__ == "__main__":
    main()
```

**Why Click?** Industry-standard CLI framework, clean decorator syntax, built-in help generation.

**Why Rich?** Beautiful terminal output, progress indicators, tables - professional UX.

**Why exit codes?** CI/CD integration - non-zero exit code fails automation pipelines.

---

### Mission 6: Package Configuration & Testing (30 mins)

**Goal**: Make package installable with `pip install .`

#### Step 6.1: Update pyproject.toml

**File**: `pyproject.toml`

```toml
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "odibi-core"
version = "1.0.0"
description = "Config-driven data engineering framework"
authors = [{name = "Henry Odibi"}]
license = {text = "MIT"}
readme = "README.md"
requires-python = ">=3.8"
dependencies = [
    "pandas>=1.5.0",
    "duckdb>=0.8.0",
    "click>=8.1.0",
    "rich>=13.0.0",
]

[project.optional-dependencies]
all = [
    "pyspark>=3.3.0",
    "azure-storage-blob>=12.0.0",
    "boto3>=1.26.0",
]
dev = [
    "pytest>=7.0.0",
    "black>=22.0.0",
    "mypy>=0.950",
]

[project.scripts]
odibi = "odibi_core.cli:main"

[tool.setuptools.packages.find]
include = ["odibi_core*"]
```

**Why [project.scripts]?** Creates CLI entry point, installs `odibi` command to PATH.

**Why optional-dependencies?** Users can install only what they need (e.g., pandas-only environments don't need Spark).

#### Step 6.2: Install Package

```bash
cd odibi_core
pip install -e ".[all]"
```

**Verification**:
```bash
odibi --help
odibi version
```

---

### Mission 7: Create Verification Script (20 mins)

**Goal**: Automated testing of all Phase 9 components

#### Step 7.1: Create Verification Script

**File**: `verify_phase9.py` (in project root)

```python[demo]
"""
Verification script for ODIBI CORE Phase 9 (SDK & CLI).

Tests:
1. Version module imports
2. SDK imports (ODIBI, Pipeline, PipelineResult)
3. ConfigValidator functionality
4. CLI module availability
5. Manifest.json structure
6. pyproject.toml configuration
"""

import sys
import json
from pathlib import Path


def test_version_module():
    """Test version module."""
    print("Testing version module...")
    
    from odibi_core.__version__ import __version__, __phase__, __supported_engines__
    
    assert __version__ == "1.0.0", f"Expected version 1.0.0, got {__version__}"
    assert __phase__ == "v1.0-phase9", f"Expected phase v1.0-phase9, got {__phase__}"
    assert "pandas" in __supported_engines__, "pandas not in supported engines"
    assert "spark" in __supported_engines__, "spark not in supported engines"
    
    print(f"  ‚úÖ Version: {__version__}")
    print(f"  ‚úÖ Phase: {__phase__}")
    print(f"  ‚úÖ Engines: {__supported_engines__}")
    print()


def test_sdk_imports():
    """Test SDK module imports."""
    print("Testing SDK imports...")
    
    from odibi_core.sdk import ODIBI, Pipeline, PipelineResult
    
    assert ODIBI is not None, "ODIBI class not found"
    assert Pipeline is not None, "Pipeline class not found"
    assert PipelineResult is not None, "PipelineResult class not found"
    
    # Test static methods exist
    assert hasattr(ODIBI, "run"), "ODIBI.run() not found"
    assert hasattr(ODIBI, "validate"), "ODIBI.validate() not found"
    assert hasattr(ODIBI, "version"), "ODIBI.version() not found"
    
    version = ODIBI.version()
    assert version == "1.0.0", f"Expected version 1.0.0, got {version}"
    
    print("  ‚úÖ ODIBI imported")
    print("  ‚úÖ Pipeline imported")
    print("  ‚úÖ PipelineResult imported")
    print(f"  ‚úÖ ODIBI.version() = {version}")
    print()


def test_config_validator():
    """Test ConfigValidator."""
    print("Testing ConfigValidator...")
    
    from odibi_core.sdk import ConfigValidator
    from odibi_core.sdk.config_validator import ValidationLevel
    
    validator = ConfigValidator()
    assert validator is not None, "ConfigValidator not instantiated"
    assert hasattr(validator, "validate"), "validate() method not found"
    assert hasattr(validator, "issues"), "issues attribute not found"
    
    # Test required fields
    assert "layer" in validator.REQUIRED_FIELDS, "layer not in REQUIRED_FIELDS"
    assert "name" in validator.REQUIRED_FIELDS, "name not in REQUIRED_FIELDS"
    assert "type" in validator.REQUIRED_FIELDS, "type not in REQUIRED_FIELDS"
    
    print("  ‚úÖ ConfigValidator instantiated")
    print("  ‚úÖ ValidationLevel enum available")
    print(f"  ‚úÖ Required fields: {validator.REQUIRED_FIELDS}")
    print()


def test_cli_module():
    """Test CLI module."""
    print("Testing CLI module...")
    
    try:
        from odibi_core import cli
        assert hasattr(cli, "main"), "main() function not found in cli module"
        print("  ‚úÖ CLI module available")
        print("  ‚úÖ main() function exists")
    except ImportError:
        print("  ‚ö†Ô∏è  CLI module not found (install click and rich)")
    print()


def test_manifest():
    """Test manifest.json."""
    print("Testing manifest.json...")
    
    manifest_path = Path(__file__).parent / "manifest.json"
    
    if not manifest_path.exists():
        # Try odibi_core directory
        manifest_path = Path(__file__).parent / "odibi_core" / "manifest.json"
    
    assert manifest_path.exists(), f"manifest.json not found at {manifest_path}"
    
    with open(manifest_path, "r") as f:
        manifest = json.load(f)
    
    assert manifest["version"] == "1.0.0", f"Expected version 1.0.0 in manifest"
    assert manifest["phase"] == "v1.0-phase9", f"Expected phase v1.0-phase9 in manifest"
    assert manifest["features"]["sdk"] is True, "SDK feature not enabled in manifest"
    assert manifest["features"]["cli"] is True, "CLI feature not enabled in manifest"
    
    print(f"  ‚úÖ Manifest found: {manifest_path}")
    print(f"  ‚úÖ Version: {manifest['version']}")
    print(f"  ‚úÖ Phase: {manifest['phase']}")
    print(f"  ‚úÖ SDK feature enabled: {manifest['features']['sdk']}")
    print()


def test_pyproject():
    """Test pyproject.toml."""
    print("Testing pyproject.toml...")
    
    pyproject_path = Path(__file__).parent / "odibi_core" / "pyproject.toml"
    
    if not pyproject_path.exists():
        # Try parent directory
        pyproject_path = Path(__file__).parent / "pyproject.toml"
    
    assert pyproject_path.exists(), f"pyproject.toml not found at {pyproject_path}"
    
    content = pyproject_path.read_text()
    
    assert "[project.scripts]" in content, "[project.scripts] section not found"
    assert "odibi = " in content, "odibi CLI entry point not defined"
    assert "click" in content, "click dependency not found"
    assert "rich" in content, "rich dependency not found"
    
    print(f"  ‚úÖ pyproject.toml found: {pyproject_path}")
    print("  ‚úÖ CLI entry point defined")
    print("  ‚úÖ click dependency present")
    print("  ‚úÖ rich dependency present")
    print()


def main():
    """Run all tests."""
    print("=" * 60)
    print("ODIBI CORE Phase 9 Verification")
    print("=" * 60)
    print()
    
    tests = [
        ("Version Module", test_version_module),
        ("SDK Imports", test_sdk_imports),
        ("ConfigValidator", test_config_validator),
        ("CLI Module", test_cli_module),
        ("Manifest", test_manifest),
        ("pyproject.toml", test_pyproject),
    ]
    
    passed = 0
    failed = 0
    
    for name, test_func in tests:
        try:
            test_func()
            passed += 1
        except AssertionError as e:
            print(f"  ‚ùå FAILED: {e}")
            failed += 1
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
            failed += 1
    
    print("=" * 60)
    print("Test Summary")
    print("=" * 60)
    
    for name, _ in tests:
        status = "‚úÖ PASS" if name not in [t[0] for t in tests[passed:]] else "‚ùå FAIL"
        print(f"{status}: {name}")
    
    print()
    print(f"Total: {passed}/{len(tests)} tests passed")
    print()
    
    if failed == 0:
        print("üéâ Phase 9 verification SUCCESSFUL!")
        sys.exit(0)
    else:
        print(f"‚ùå {failed} test(s) failed")
        sys.exit(1)


if __name__ == "__main__":
    main()
```

**Why verification script?** Automated regression testing, quick validation after changes, CI/CD integration.

#### Step 7.2: Run Verification

```bash
python verify_phase9.py
```

**Expected output**:
```
============================================================
ODIBI CORE Phase 9 Verification
============================================================

Testing version module...
  ‚úÖ Version: 1.0.0
  ‚úÖ Phase: v1.0-phase9
  ‚úÖ Engines: ['pandas', 'spark']

Testing SDK imports...
  ‚úÖ ODIBI imported
  ‚úÖ Pipeline imported
  ‚úÖ PipelineResult imported
  ‚úÖ ODIBI.version() = 1.0.0

Testing ConfigValidator...
  ‚úÖ ConfigValidator instantiated
  ‚úÖ ValidationLevel enum available
  ‚úÖ Required fields: {'type', 'name', 'layer'}

Testing CLI module...
  ‚úÖ CLI module available
  ‚úÖ main() function exists

Testing manifest.json...
  ‚úÖ Manifest found: d:\projects\odibi_core\manifest.json
  ‚úÖ Version: 1.0.0
  ‚úÖ Phase: v1.0-phase9
  ‚úÖ SDK feature enabled: True

Testing pyproject.toml...
  ‚úÖ pyproject.toml found: d:\projects\odibi_core\pyproject.toml
  ‚úÖ CLI entry point defined
  ‚úÖ click dependency present
  ‚úÖ rich dependency present

============================================================
Test Summary
============================================================
‚úÖ PASS: Version Module
‚úÖ PASS: SDK Imports
‚úÖ PASS: ConfigValidator
‚úÖ PASS: CLI Module
‚úÖ PASS: Manifest
‚úÖ PASS: pyproject.toml

Total: 6/6 tests passed

üéâ Phase 9 verification SUCCESSFUL!
```

---

## üéì Design Patterns & Best Practices

### Pattern 1: SDK Wrapper (Not Replacement)

**Goal**: Provide simple API without limiting advanced users

**Implementation**:
```python
# SDK approach (simple)
from odibi_core.sdk import ODIBI
result = ODIBI.run("pipeline.json")

# Core module approach (advanced, still works!)
from odibi_core.core import ConfigLoader, DAGExecutor
loader = ConfigLoader()
# ... full control
```

**Why?** Onboard new users quickly, but don't constrain power users.

### Pattern 2: Fluent API (Method Chaining)

**Goal**: Readable configuration code

**Implementation**:
```python
pipeline = (Pipeline.from_config("config.json")
            .set_engine("pandas")
            .set_secrets(secrets)
            .set_parallelism(8))
```

**Key**: Each setter returns `self`

**Why?** Reduces variable declarations, improves readability.

### Pattern 3: Static Utility Class

**Goal**: Group related functions without instantiation

**Implementation**:
```python
class ODIBI:
    @staticmethod
    def run(...): ...
    
    @staticmethod
    def validate(...): ...
    
    @staticmethod
    def version(...): ...
```

**Why?** Follows `requests.get()`, `json.dumps()` pattern - familiar to Python developers.

### Pattern 4: Alternate Constructors (@classmethod)

**Goal**: Multiple ways to create objects

**Implementation**:
```python
# From config file
pipeline = Pipeline.from_config("pipeline.json")

# From steps list
pipeline = Pipeline(steps, name="custom")
```

**Why?** Common Python idiom, provides flexibility.

### Pattern 5: CLI Exit Codes for CI/CD

**Goal**: Integrate with automation pipelines

**Implementation**:
```python
if result.is_success():
    sys.exit(0)  # Success
else:
    sys.exit(1)  # Failure
```

**CI/CD usage**:
```yaml
- name: Run Pipeline
  run: odibi run --config pipeline.json
  # Job fails if exit code != 0
```

**Why?** Standard Unix convention enables CI/CD integration.

---

## üß™ Testing Patterns

### Test 1: SDK Import Test

```python
from odibi_core.sdk import ODIBI, Pipeline

# Should not raise ImportError
```

### Test 2: SDK Execution Test

```python
result = ODIBI.run("test_config.json", engine="pandas")
assert result.is_success()
assert result.success_count > 0
```

### Test 3: Config Validation Test

```python
validator = ConfigValidator()
is_valid = validator.validate("invalid_config.json")
assert not is_valid
assert len(validator.issues) > 0
```

### Test 4: CLI Help Test

```bash
odibi --help
odibi run --help
odibi validate --help
```

### Test 5: Version Consistency Test

```python
from odibi_core.__version__ import __version__
from odibi_core.sdk import ODIBI
import json

# Check version consistency
assert ODIBI.version() == __version__

with open("manifest.json") as f:
    manifest = json.load(f)
assert manifest["version"] == __version__
```

---

## üìä Architecture Comparison

### Before Phase 9 (Phases 1-8)

**Execution**:
```python
# 6 imports, 5 objects to manage
from odibi_core.core import ConfigLoader, Orchestrator
from odibi_core.engine import PandasEngineContext
from odibi_core.core import Tracker, EventEmitter

loader = ConfigLoader()
steps = loader.load("pipeline.json")
context = PandasEngineContext(secrets=secrets)
tracker = Tracker()
events = EventEmitter()
orchestrator = Orchestrator(steps, context, tracker, events)
result = orchestrator.run()
```

**Validation**: None (manual checking)

**CLI**: Python script required

### After Phase 9

**Execution (SDK)**:
```python
# 1 import, 1 line
from odibi_core.sdk import ODIBI
result = ODIBI.run("pipeline.json", secrets=secrets)
```

**Execution (CLI)**:
```bash
# No Python required!
odibi run --config pipeline.json
```

**Validation**:
```bash
odibi validate --config pipeline.json
```

**Version Check**:
```bash
odibi version
```

---

## üöÄ Real-World Usage Examples

### Example 1: CI/CD Pipeline

**GitHub Actions** (`.github/workflows/pipeline.yml`):
```yaml
name: Run ODIBI Pipeline

on: [push, pull_request]

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      
      - name: Install ODIBI CORE
        run: |
          pip install -e ".[all]"
      
      - name: Validate Config
        run: |
          odibi validate --config pipeline.json --strict
      
      - name: Run Pipeline
        run: |
          odibi run --config pipeline.json --engine pandas --verbose
        env:
          DB_PASS: ${{ secrets.DB_PASS }}
          API_KEY: ${{ secrets.API_KEY }}
```

### Example 2: Environment-Based Execution

**Python script** (`run_pipeline.py`):
```python
import os
from odibi_core.sdk import Pipeline

# Select config by environment
env = os.getenv("ENV", "dev")
config_file = f"configs/{env}_pipeline.json"

# Load secrets from environment
secrets = {
    "db_host": os.getenv("DB_HOST"),
    "db_pass": os.getenv("DB_PASS"),
    "api_key": os.getenv("API_KEY"),
}

# Execute
pipeline = Pipeline.from_config(config_file)
pipeline.set_engine("pandas").set_secrets(secrets)
result = pipeline.execute()

# Log results
if result.is_success():
    print(f"‚úÖ Pipeline succeeded: {result.total_duration_ms:.2f}ms")
else:
    print(f"‚ùå Pipeline failed: {result.failed_count} nodes")
    sys.exit(1)
```

**Usage**:
```bash
ENV=prod python run_pipeline.py
```

### Example 3: Multi-Pipeline Orchestration

**Python script** (`medallion_pipeline.py`):
```python
from odibi_core.sdk import Pipeline

# Execute medallion architecture: Bronze ‚Üí Silver ‚Üí Gold
pipelines = [
    ("Bronze", "bronze_ingestion.json"),
    ("Silver", "silver_transformation.json"),
    ("Gold", "gold_aggregation.json"),
]

results = []
for layer, config in pipelines:
    print(f"Executing {layer} layer...")
    
    pipeline = Pipeline.from_config(config)
    pipeline.set_engine("pandas").set_parallelism(8)
    result = pipeline.execute()
    
    if not result.is_success():
        print(f"‚ùå {layer} layer failed")
        break
    
    results.append((layer, result))
    print(f"‚úÖ {layer} layer completed: {result.total_duration_ms:.2f}ms")

# Summary
total_duration = sum(r.total_duration_ms for _, r in results)
print(f"\nMedallion pipeline completed in {total_duration:.2f}ms")
```

### Example 4: Programmatic Pipeline Creation

**Python script** (`dynamic_pipeline.py`):
```python
from odibi_core.sdk import Pipeline
from odibi_core.core.node import Step

# Build pipeline dynamically based on user input
data_sources = ["sales.csv", "inventory.csv", "customers.csv"]

steps = []
for i, source in enumerate(data_sources):
    # Ingest step
    steps.append(Step(
        layer="ingest",
        name=f"read_{source.split('.')[0]}",
        type="config_op",
        engine="pandas",
        value=f"data/{source}",
        outputs={"data": f"raw_{i}"}
    ))
    
    # Transform step
    steps.append(Step(
        layer="transform",
        name=f"clean_{source.split('.')[0]}",
        type="sql",
        engine="pandas",
        value=f"SELECT * FROM data WHERE id IS NOT NULL",
        inputs={"data": f"raw_{i}"},
        outputs={"data": f"clean_{i}"}
    ))

# Execute
pipeline = Pipeline(steps, name="dynamic_multi_source")
result = pipeline.execute()
```

---

## üéØ Phase 9 Success Criteria

All objectives met:

- ‚úÖ **SDK Layer**: Clean `ODIBI` and `Pipeline` API
- ‚úÖ **CLI Tool**: `odibi` command with run, validate, version
- ‚úÖ **Config Validator**: Pre-execution validation with circular dep detection
- ‚úÖ **Version Tracking**: `__version__.py` with metadata
- ‚úÖ **Manifest**: `manifest.json` for capability discovery
- ‚úÖ **Packaging**: Clean `pip install .` support
- ‚úÖ **Documentation**: Complete walkthrough and examples
- ‚úÖ **Testing**: Verification script with 6 passing tests
- ‚úÖ **Backward Compatibility**: 100% - all Phase 1-8 code still works

---

## üìù What You Built

### Files Created

| File | Lines | Purpose |
|------|-------|---------|
| `odibi_core/__version__.py` | 10 | Version tracking |
| `odibi_core/sdk/__init__.py` | 320 | SDK layer (ODIBI, Pipeline) |
| `odibi_core/sdk/config_validator.py` | 350 | Config validation |
| `odibi_core/sdk/doc_generator.py` | 30 | Doc generator placeholder |
| `odibi_core/cli.py` | 310 | CLI tool |
| `manifest.json` | 60 | Framework metadata |
| `verify_phase9.py` | 200 | Verification script |

**Total**: ~1,280 lines of production code

### Files Modified

- `odibi_core/__init__.py` - Import from `__version__.py`
- `pyproject.toml` - Added CLI entry point and dependencies
- `README.md` - Updated with CLI/SDK examples

### Capabilities Added

- ‚úÖ One-line pipeline execution
- ‚úÖ Command-line tools
- ‚úÖ Pre-execution validation
- ‚úÖ Version management
- ‚úÖ Framework metadata

---

## üîÆ What's Next?

### Phase 10: Learning Ecosystem & Community

- Auto-generated API documentation (Sphinx or MkDocs)
- LearnODIBI tutorial platform
- Migration guide (odibi_de_v2 ‚Üí odibi_core)
- Sample project templates
- Community building (GitHub Discussions, talks)

---

## üéì Key Learnings

### Design Principles Applied

1. **Wrapper, Not Replacement**: SDK simplifies common cases but doesn't limit advanced users
2. **Fluent Interface**: Method chaining improves code readability
3. **Static Utilities**: Group related functions without instantiation overhead
4. **Alternate Constructors**: @classmethod provides flexibility
5. **Exit Codes**: Enable CI/CD integration
6. **Dependency Injection**: CLI depends on SDK depends on core (clean layers)

### Testing Insights

- **Version Consistency**: Version must match across `__version__.py`, `manifest.json`, `pyproject.toml`
- **Import Tests**: Catch breaking changes early
- **CLI Help Tests**: Ensure user-facing documentation is correct
- **Exit Code Tests**: Verify CI/CD compatibility

### Production Readiness Checklist

- ‚úÖ Clean installation (`pip install .`)
- ‚úÖ CLI entry point working
- ‚úÖ Help documentation (`--help` flags)
- ‚úÖ Error messages are clear
- ‚úÖ Exit codes follow Unix conventions
- ‚úÖ Validation before execution
- ‚úÖ Backward compatibility maintained

---

## üìö Additional Resources

- **Click Documentation**: https://click.palletsprojects.com/
- **Rich Documentation**: https://rich.readthedocs.io/
- **Python Packaging Guide**: https://packaging.python.org/
- **Semantic Versioning**: https://semver.org/
- **Unix Exit Codes**: https://tldp.org/LDP/abs/html/exitcodes.html

---

**Phase 9 Complete!** üéâ

You've transformed ODIBI CORE from a development framework into a production-ready SDK with CLI tools. The framework is now installable, user-friendly, and ready for widespread adoption.

**Next**: Phase 10 - Learning Ecosystem & Community

---

*End of Phase 9 Developer Walkthrough*
